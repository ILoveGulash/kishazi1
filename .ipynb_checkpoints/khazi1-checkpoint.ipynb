{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import bs4\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [250 250 250]\n",
      " [253 253 253]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [252 252 252]\n",
      " [255 255 255]\n",
      " [217 217 217]\n",
      " [111 111 111]\n",
      " [  0   0   0]\n",
      " [  0   0   0]\n",
      " [  3   3   3]\n",
      " [ 15  15  15]\n",
      " [  0   0   0]\n",
      " [  0   0   0]\n",
      " [  0   0   0]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [ 14  14  14]\n",
      " [  0   0   0]\n",
      " [  1   1   1]\n",
      " [ 11  11  11]\n",
      " [  2   2   2]\n",
      " [ 24  24  24]\n",
      " [135 135 135]\n",
      " [253 253 253]\n",
      " [243 243 243]\n",
      " [255 255 255]\n",
      " [252 252 252]\n",
      " [245 245 245]\n",
      " [255 255 255]\n",
      " [251 251 251]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [250 250 250]\n",
      " [252 252 252]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 250 255]\n",
      " [253 255 235]\n",
      " [250 255 254]\n",
      " [252 255 239]\n",
      " [254 242 255]\n",
      " [246 217 211]\n",
      " [241 219  35]\n",
      " [247 213   4]\n",
      " [255 194   7]\n",
      " [246 207  28]\n",
      " [228 212 137]\n",
      " [223 218 240]\n",
      " [231 236 242]\n",
      " [238 243 220]\n",
      " [236 237 231]\n",
      " [232 232 234]\n",
      " [229 229 229]\n",
      " [229 229 229]\n",
      " [227 227 227]\n",
      " [225 225 225]\n",
      " [223 223 223]\n",
      " [221 221 221]\n",
      " [220 220 220]\n",
      " [219 219 219]\n",
      " [235 235 235]\n",
      " [195 195 195]\n",
      " [211 211 211]\n",
      " [234 234 234]\n",
      " [202 202 202]\n",
      " [188 188 188]\n",
      " [206 206 206]\n",
      " [206 206 206]\n",
      " [207 207 207]\n",
      " [207 207 207]\n",
      " [162 162 162]\n",
      " [ 15  15  15]\n",
      " [ 21  21  21]\n",
      " [  0   0   0]\n",
      " [  0   0   0]\n",
      " [  8   8   8]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [  2   2   2]\n",
      " [  5   5   5]\n",
      " [  7   7   7]\n",
      " [  4   4   4]\n",
      " [  4   4   4]\n",
      " [  0   0   0]\n",
      " [  0   0   0]\n",
      " [  0   0   0]\n",
      " [ 29  29  29]\n",
      " [123 123 123]\n",
      " [206 206 206]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [251 251 251]\n",
      " [253 253 253]\n",
      " [254 254 254]\n",
      " [251 251 251]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]]\n",
      "[[255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [171 229 170]\n",
      " [ 86 165  82]\n",
      " [ 72 178  70]\n",
      " [ 71 185  71]\n",
      " [ 78 185  79]\n",
      " [ 79 179  80]\n",
      " [ 75 178  73]\n",
      " [ 71 179  67]\n",
      " [ 75 181  73]\n",
      " [ 75 181  73]\n",
      " [ 75 181  73]\n",
      " [ 75 181  73]\n",
      " [ 75 181  73]\n",
      " [ 75 181  73]\n",
      " [ 75 181  73]\n",
      " [ 75 181  73]\n",
      " [ 70 188  76]\n",
      " [ 73 176  67]\n",
      " [ 77 170  63]\n",
      " [ 77 183  72]\n",
      " [ 91 184  80]\n",
      " [ 62 162  64]\n",
      " [147  66  37]\n",
      " [179  21  22]\n",
      " [183  20  21]\n",
      " [185  21  22]\n",
      " [188  22  24]\n",
      " [191  22  25]\n",
      " [195  25  28]\n",
      " [199  24  29]\n",
      " [202  25  31]\n",
      " [203  26  32]\n",
      " [204  54  79]\n",
      " [243 231 207]\n",
      " [219 222 201]\n",
      " [226 240 241]\n",
      " [255 242 255]\n",
      " [ 78 145 190]\n",
      " [ 91 150 206]\n",
      " [ 72 150 198]\n",
      " [ 79 140 195]\n",
      " [ 87 146 202]\n",
      " [ 89 148 204]\n",
      " [ 86 146 200]\n",
      " [ 90 149 205]\n",
      " [ 98 157 213]\n",
      " [ 96 157 214]\n",
      " [ 88 150 209]\n",
      " [ 97 152 209]\n",
      " [ 97 152 209]\n",
      " [ 98 153 210]\n",
      " [ 99 154 211]\n",
      " [100 155 212]\n",
      " [100 155 212]\n",
      " [101 156 213]\n",
      " [101 156 213]\n",
      " [101 156 213]\n",
      " [101 156 213]\n",
      " [100 155 212]\n",
      " [100 155 212]\n",
      " [ 99 154 211]\n",
      " [ 98 153 210]\n",
      " [ 97 152 209]\n",
      " [ 97 152 209]\n",
      " [ 91 154 207]\n",
      " [ 98 159 213]\n",
      " [ 97 157 211]\n",
      " [ 88 148 202]\n",
      " [ 84 143 199]\n",
      " [ 89 148 206]\n",
      " [ 90 149 209]\n",
      " [ 84 144 206]\n",
      " [ 82 147 213]\n",
      " [ 79 140 205]\n",
      " [140 172 221]\n",
      " [242 248 236]\n",
      " [234 241 255]\n",
      " [205 234 204]\n",
      " [234 220 207]\n",
      " [254 200  40]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [251 207  10]\n",
      " [247 207   0]\n",
      " [251 211  17]\n",
      " [255 215  40]\n",
      " [250 204   0]\n",
      " [254 202   0]\n",
      " [253 206   2]\n",
      " [240 212  27]\n",
      " [255 236 152]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]]\n",
      "[[255 255 251]\n",
      " [255 252 248]\n",
      " [255 250 243]\n",
      " [255 244 225]\n",
      " [226 178 142]\n",
      " [155  95  35]\n",
      " [140  72   0]\n",
      " [171  98   3]\n",
      " [235 146  28]\n",
      " [244 167  25]\n",
      " [255 201  42]\n",
      " [255 212  66]\n",
      " [255 208  92]\n",
      " [255 219 124]\n",
      " [255 220 119]\n",
      " [232 196  82]\n",
      " [253 207  95]\n",
      " [241 204 115]\n",
      " [232 210 160]\n",
      " [255 255 239]\n",
      " [173 174 179]\n",
      " [ 31  33  45]\n",
      " [ 95  94  99]\n",
      " [ 98  95  90]\n",
      " [ 88  87  92]\n",
      " [111 111 113]\n",
      " [106 104 105]\n",
      " [ 54  52  53]\n",
      " [ 42  41  39]\n",
      " [  1   0   0]\n",
      " [138 138 138]\n",
      " [254 255 255]\n",
      " [232 207 143]\n",
      " [226 185 106]\n",
      " [245 187  90]\n",
      " [255 201  95]\n",
      " [255 198  94]\n",
      " [248 194  96]\n",
      " [252 196  99]\n",
      " [254 194  96]\n",
      " [255 193  96]\n",
      " [255 193  96]\n",
      " [255 193  96]\n",
      " [255 192  95]\n",
      " [255 192  95]\n",
      " [255 192  95]\n",
      " [254 191  94]\n",
      " [254 191  94]\n",
      " [254 192  79]\n",
      " [251 188  95]\n",
      " [255 188 110]\n",
      " [255 189 102]\n",
      " [255 184  80]\n",
      " [252 183  80]\n",
      " [251 198 128]\n",
      " [255 220 181]\n",
      " [255 235 195]\n",
      " [255 237 189]\n",
      " [250 243 188]\n",
      " [250 246 198]\n",
      " [254 245 202]\n",
      " [255 237 176]\n",
      " [254 225 125]\n",
      " [250 216  82]\n",
      " [255 219  87]\n",
      " [255 219  87]\n",
      " [255 219  87]\n",
      " [255 219  87]\n",
      " [255 219  87]\n",
      " [255 219  87]\n",
      " [255 219  87]\n",
      " [255 219  87]\n",
      " [255 222  92]\n",
      " [255 222  89]\n",
      " [248 219  83]\n",
      " [244 217  78]\n",
      " [253 221  86]\n",
      " [255 222  94]\n",
      " [255 204  88]\n",
      " [246 182  72]\n",
      " [240 168  66]\n",
      " [255 156 104]\n",
      " [255 157  90]\n",
      " [251 172  77]\n",
      " [243 164  85]\n",
      " [255 155  66]\n",
      " [244 121  17]\n",
      " [190  72   2]\n",
      " [172 116  19]\n",
      " [255 222 114]\n",
      " [255 230 107]\n",
      " [255 241 112]\n",
      " [255 238 105]\n",
      " [255 227  81]\n",
      " [255 213  43]\n",
      " [241 186   0]\n",
      " [175  95   0]\n",
      " [114  64  31]\n",
      " [131 118 125]\n",
      " [188 192 178]\n",
      " [191 173 135]\n",
      " [154  96  58]\n",
      " [148  60  14]\n",
      " [166  68   0]\n",
      " [234 106   0]\n",
      " [233 123   8]\n",
      " [244 159  66]\n",
      " [255 204 153]\n",
      " [255 235 202]\n",
      " [255 242 203]\n",
      " [251 235 202]\n",
      " [247 229 217]\n",
      " [255 244 218]\n",
      " [232 241 212]\n",
      " [224 241 225]\n",
      " [251 234 227]\n",
      " [255 202 162]\n",
      " [255 144  55]\n",
      " [210  85   0]\n",
      " [172  48   0]\n",
      " [113  89  77]\n",
      " [124 115 110]\n",
      " [149 157 159]\n",
      " [186 198 198]\n",
      " [220 225 218]\n",
      " [243 241 229]\n",
      " [255 252 245]\n",
      " [255 255 255]]\n",
      "[[253 253 253]\n",
      " [254 254 254]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [252 252 252]\n",
      " [249 249 249]\n",
      " [247 247 247]\n",
      " [249 251 240]\n",
      " [224 225 229]\n",
      " [214 217 232]\n",
      " [205 204 210]\n",
      " [195 184 164]\n",
      " [154 126  79]\n",
      " [151 103  41]\n",
      " [163 100  33]\n",
      " [168 103  19]\n",
      " [177 111  24]\n",
      " [188 122  28]\n",
      " [197 128  27]\n",
      " [202 130  20]\n",
      " [207 130  12]\n",
      " [214 134   9]\n",
      " [221 138  10]\n",
      " [225 149  12]\n",
      " [229 153  16]\n",
      " [234 158  21]\n",
      " [239 163  25]\n",
      " [243 167  29]\n",
      " [245 169  31]\n",
      " [246 171  30]\n",
      " [245 170  29]\n",
      " [254 163  30]\n",
      " [243 191  19]\n",
      " [238 188  39]\n",
      " [250 175  24]\n",
      " [251 173   0]\n",
      " [255 194  61]\n",
      " [255 211 115]\n",
      " [236 185  16]\n",
      " [255 206  67]\n",
      " [252 196  83]\n",
      " [231 183  72]\n",
      " [255 213 121]\n",
      " [255 205 109]\n",
      " [247 190  13]\n",
      " [255 183   3]\n",
      " [255 168   9]\n",
      " [245 162   0]\n",
      " [255 205  41]\n",
      " [248 204 109]\n",
      " [235 205  45]\n",
      " [251 202  48]\n",
      " [255 185  80]\n",
      " [255 191  17]\n",
      " [250 183  43]\n",
      " [240 198  50]\n",
      " [246 193  53]\n",
      " [247 189   0]\n",
      " [243 179   0]\n",
      " [255 191   8]\n",
      " [244 177   0]\n",
      " [241 184   9]\n",
      " [255 201  85]\n",
      " [255 215 108]\n",
      " [239 204  78]\n",
      " [255 177   0]\n",
      " [252 191   0]\n",
      " [255 184  55]\n",
      " [255 178  29]\n",
      " [255 218 130]\n",
      " [241 193  95]\n",
      " [240 171   8]\n",
      " [245 185  26]\n",
      " [237 186  33]\n",
      " [255 193 106]\n",
      " [255 205  91]\n",
      " [249 186  19]\n",
      " [248 184  87]\n",
      " [255 205 137]\n",
      " [237 182  19]\n",
      " [238 182  71]\n",
      " [230 178  68]\n",
      " [223 173   0]\n",
      " [225 168   0]\n",
      " [222 164   0]\n",
      " [219 168   0]\n",
      " [235 178 125]\n",
      " [233 170  29]\n",
      " [236 168  57]\n",
      " [220 158  25]\n",
      " [216 154  21]\n",
      " [253 185 100]\n",
      " [228 162  65]\n",
      " [204 147   8]\n",
      " [217 157  35]\n",
      " [207 143  17]\n",
      " [204 143  18]\n",
      " [202 140  19]\n",
      " [199 139  19]\n",
      " [196 134  21]\n",
      " [193 133  21]\n",
      " [191 130  23]\n",
      " [188 130  23]\n",
      " [182 124  24]\n",
      " [180 122  25]\n",
      " [175 119  26]\n",
      " [169 115  27]\n",
      " [163 110  30]\n",
      " [158 106  31]\n",
      " [153 103  32]\n",
      " [151 102  35]\n",
      " [144  97  43]\n",
      " [149 120  86]\n",
      " [203 195 184]\n",
      " [204 205 209]\n",
      " [220 219 224]\n",
      " [243 239 240]\n",
      " [245 245 247]\n",
      " [246 250 253]\n",
      " [253 253 253]\n",
      " [253 253 253]\n",
      " [253 253 253]\n",
      " [254 254 254]\n",
      " [254 254 254]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]]\n",
      "[[251 255 255]\n",
      " [255 252 246]\n",
      " [254 232 218]\n",
      " [214  88  91]\n",
      " [221  86  82]\n",
      " [232  79  74]\n",
      " [209  92  74]\n",
      " [223  86  76]\n",
      " [215  88  71]\n",
      " [229  86  78]\n",
      " [223  92  84]\n",
      " [222  85  77]\n",
      " [217  86  76]\n",
      " [234 101 106]\n",
      " [247 255 241]\n",
      " [255 254 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [247 255 255]\n",
      " [255 243 251]\n",
      " [206 114 129]\n",
      " [206  86  98]\n",
      " [216  80  94]\n",
      " [214  67  86]\n",
      " [217  79 102]\n",
      " [200  81 101]\n",
      " [216  70  89]\n",
      " [217  86 100]\n",
      " [197  75  86]\n",
      " [208  75  92]\n",
      " [211  72  95]\n",
      " [213 101 123]\n",
      " [248 201 207]\n",
      " [251 255 251]\n",
      " [252 255 255]\n",
      " [254 255 255]\n",
      " [248 248 248]\n",
      " [246 244 245]\n",
      " [247 245 246]\n",
      " [255 255 255]\n",
      " [254 255 255]\n",
      " [252 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [245 251 247]\n",
      " [251 255 252]\n",
      " [254 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 253]\n",
      " [254 254 252]\n",
      " [255 255 251]\n",
      " [247 255 255]\n",
      " [255 249 255]\n",
      " [183 134 181]\n",
      " [159  75 150]\n",
      " [166  63 154]\n",
      " [165  61 156]\n",
      " [148  55 146]\n",
      " [145  63 148]\n",
      " [145  64 166]\n",
      " [148  67 160]\n",
      " [151  70 162]\n",
      " [141  63 162]\n",
      " [140  72 175]\n",
      " [120  71 153]\n",
      " [205 180 219]\n",
      " [255 252 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 255 255]\n",
      " [255 254 250]\n",
      " [255 254 255]\n",
      " [223 193 231]\n",
      " [148  65 143]\n",
      " [160  60 158]\n",
      " [147  67 162]\n",
      " [141  68 158]\n",
      " [154  63 158]\n",
      " [145  66 158]\n",
      " [143  70 160]\n",
      " [138  67 159]\n",
      " [143  69 168]\n",
      " [135  64 160]\n",
      " [168 115 183]\n",
      " [255 247 255]\n",
      " [251 255 244]]\n"
     ]
    }
   ],
   "source": [
    "#1. loading and printing images\n",
    "\n",
    "images = []\n",
    "for i in range(1,6):\n",
    "    images.append(mpimg.imread(f'img{i}.jpeg'))\n",
    "    print(images[i-1][54])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showimage = Image.fromarray(images[0], 'RGB')\n",
    "#showimage.show()\n",
    "\n",
    "#for displaying an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. extracting mean, variance, and the channels\n",
    "\n",
    "imagesR, imagesG, imagesB = [],[],[]\n",
    "meanR, meanG, meanB = [],[],[]\n",
    "varR, varG, varB = [],[],[]\n",
    "\n",
    "means = []\n",
    "variances = []\n",
    "i = 0\n",
    "for image in images:\n",
    "    imagesR.append(np.array(image[:,:,0]))\n",
    "    imagesG.append(np.array(image[:,:,1]))\n",
    "    imagesB.append(np.array(image[:,:,2]))\n",
    "    meanR.append(np.mean(imagesR[i]))\n",
    "    meanG.append(np.mean(imagesG[i]))\n",
    "    meanB.append(np.mean(imagesB[i]))\n",
    "    varR.append(np.var(imagesR[i]))\n",
    "    varG.append(np.var(imagesG[i]))\n",
    "    varB.append(np.var(imagesB[i]))\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n\\n\\n        The future of deep learning\\n        \\n        \\n        \\n\\n        \\n        \\n\\n\\n        \\n\\n        \\n\\n        \\n\\n\\n\\n\\n        \\n                \\n                    The Keras Blog \\n                \\n                \\n                    Keras is a Deep Learning library for Python, that is simple, modular, and extensible.\\n                \\n                \\n                Archives\\n                    \\n                        Github\\n                    \\n                    \\n                        Documentation\\n                    \\n                    \\n                        Google Group\\n                    \\n                \\n        \\n\\n\\n\\n         The future of deep learning \\n        \\n\\n        \\n                Tue 18 July 2017\\n        \\n\\n        \\n                By Francois Chollet\\n        \\nIn Essays. \\n\\n        This post is adapted from Section 3 of Chapter 9 of my book, Deep Learning with Python (Manning Publications). \\n\\nIt is part of a series of two posts on the current limitations of deep learning, and its future. \\nYou can read the first part here: The Limitations of Deep Learning.\\n\\nGiven what we know of how deep nets work, of their limitations, and of the current state of the research landscape, \\ncan we predict where things are headed in the medium term? Here are some purely personal thoughts. Note that I don\\'t have a crystal ball, \\nso a lot of what I anticipate might fail to become reality. This is a completely speculative post. \\nI am sharing these predictions not because I expect them to be proven \\ncompletely right in the future, but because they are interesting and actionable in the present.\\nAt a high-level, the main directions in which I see promise are:\\n\\nModels closer to general-purpose computer programs, built on top of far richer primitives than our current differentiable layers\\xe2\\x80\\x94this \\nis how we will get to reasoning and abstraction, the fundamental weakness of current models.\\nNew forms of learning that make the above possible\\xe2\\x80\\x94allowing models to move away from just differentiable transforms.\\nModels that require less involvement from human engineers\\xe2\\x80\\x94it shouldn\\'t be your job to tune knobs endlessly.\\nGreater, systematic reuse of previously learned features and architectures; meta-learning systems based on reusable and modular program \\nsubroutines.\\n\\nAdditionally, do note that these considerations are not specific to the sort of supervised learning that has been the bread and butter of \\ndeep learning so far\\xe2\\x80\\x94rather, they are applicable to any form of machine learning, including unsupervised, self-supervised, and \\nreinforcement learning. It is not fundamentally important where your labels come from or what your training loop looks like; \\nthese different branches of machine learning are just different facets of a same construct.\\nLet\\'s dive in.\\nModels as programs\\nAs we noted in our previous post, a necessary transformational development that we can expect in the field of machine learning is a move away \\nfrom models that perform purely pattern recognition and can only achieve local generalization, towards models capable of abstraction and \\nreasoning, that can achieve extreme generalization. Current AI programs that are capable of basic forms of reasoning are all hard-coded \\nby human programmers: for instance, software that relies on search algorithms, graph manipulation, formal logic. \\nIn DeepMind\\'s AlphaGo, for example, most of the \"intelligence\" on display is designed and hard-coded by expert programmers (e.g. Monte-Carlo tree search); \\nlearning from data only happens in specialized submodules (value networks and policy networks). \\nBut in the future, such AI systems may well be fully learned, with no human involvement.\\nWhat could be the path to make this happen? Consider a well-known type of network: RNNs. \\nImportantly, RNNs have slightly less limitations than feedforward networks. \\nThat is because RNNs are a bit more than a mere geometric transformation: they are geometric transformations repeatedly applied inside a for \\nloop. The temporal for loop is itself hard-coded by human developers: it is a built-in assumption of the network. Naturally, RNNs are \\nstill extremely limited in what they can represent, primarily because each step they perform is still just a differentiable geometric \\ntransformation, and the way they carry information from step to step is via points in a continuous geometric space (state vectors). Now, \\nimagine neural networks that would be \"augmented\" in a similar way with programming primitives such as for loops\\xe2\\x80\\x94but not just a single \\nhard-coded for loop with a hard-coded geometric memory, rather, a large set of programming primitives that the model would be free to manipulate to expand its \\nprocessing function, such as if branches, while statements, variable creation, disk storage for long-term memory, \\nsorting operators, advanced datastructures like lists, graphs, and hashtables, and many more. \\nThe space of programs that such a network could represent would be far broader than what can be represented with current deep learning models, \\nand some of these programs could achieve superior generalization power.\\nIn a word, we will move away from having on one hand \"hard-coded algorithmic intelligence\" (handcrafted software) and on the other hand \\n\"learned geometric intelligence\" (deep learning). We will have instead a blend of formal algorithmic modules that provide reasoning and \\nabstraction capabilities, and geometric modules that provide informal intuition and pattern recognition capabilities. The whole system would be \\nlearned with little or no human involvement.\\nA related subfield of AI that I think may be about to take off in a big way is that of program synthesis, in particular neural program \\nsynthesis. Program synthesis consists in automatically generating simple programs, by using a search algorithm (possibly genetic search, as \\nin genetic programming) to explore a large space of possible programs. The search stops when a program is found that matches the required \\nspecifications, often provided as a set of input-output pairs. As you can see, is it highly reminiscent of machine learning: given \\n\"training data\" provided as input-output pairs, we find a \"program\" that matches inputs to outputs and can generalize to new inputs. The \\ndifference is that instead of learning parameter values in a hard-coded program (a neural network), \\nwe generate source code via a discrete search process.\\nI would definitely expect this subfield to see a wave of renewed interest in the next few years. In particular, I would expect the \\nemergence of a crossover subfield in-between deep learning and program synthesis, where we would not quite be generating programs in a \\ngeneral-purpose language, but rather, where we would be generating neural networks (geometric data processing flows) augmented with a \\nrich set of algorithmic primitives, such as for loops\\xe2\\x80\\x94and many others. This should be far more tractable and useful than directly \\ngenerating source code, and it would dramatically expand the scope of problems that can be solved with machine learning\\xe2\\x80\\x94the space of programs \\nthat we can generate automatically given appropriate training data. A blend of symbolic AI and geometric AI. \\nContemporary RNNs can be seen as a prehistoric ancestor to such hybrid algorithmic-geometric models.\\n\\nFigure: A learned program relying on both geometric primitives (pattern recognition, intuition) and algorithmic primitives (reasoning, search, memory).\\nBeyond backpropagation and differentiable layers\\nIf machine learning models become more like programs, then they will mostly no longer be differentiable\\xe2\\x80\\x94certainly, these programs will \\nstill leverage continuous geometric layers as subroutines, which will be differentiable, but the model as a whole would not be. As a \\nresult, using backpropagation to adjust weight values in a fixed, hard-coded network, cannot be the method of choice for training models in the \\nfuture\\xe2\\x80\\x94at least, it cannot be the whole story. We need to figure out to train non-differentiable systems efficiently. Current approaches \\ninclude genetic algorithms, \"evolution strategies\", certain reinforcement learning methods, \\nand ADMM (alternating direction method of multipliers). \\nNaturally, gradient descent is not going anywhere\\xe2\\x80\\x94gradient information will always be useful for optimizing differentiable parametric functions. But our models \\nwill certainly become increasingly more ambitious than mere differentiable parametric functions, \\nand thus their automatic development (the \"learning\" in \"machine learning\") will require more than backpropagation.\\nBesides, backpropagation is end-to-end, which is a great thing for learning good chained transformations, but is rather computationally \\ninefficient since it doesn\\'t fully leverage the modularity of deep networks. To make something more efficient, there is one universal \\nrecipe: introduce modularity and hierarchy. So we can make backprop itself more efficient by introducing decoupled training modules with some \\nsynchronization mechanism between them, organized in a hierarchical fashion. This strategy is somewhat reflected in DeepMind\\'s recent work \\non \"synthetic gradients\". I would expect more more work along these lines in the near future.\\nOne can imagine a future where models that would be globally non-differentiable (but would feature differentiable parts) would be \\ntrained\\xe2\\x80\\x94grown\\xe2\\x80\\x94using an efficient search process that would not leverage gradients, while the differentiable parts would be trained even faster \\nby taking advantage of gradients using some more efficient version of backpropagation.\\nAutomated machine learning\\nIn the future, model architectures will be learned, rather than handcrafted by engineer-artisans. Learning architectures automatically goes \\nhand in hand with the use of richer sets of primitives and program-like machine learning models.\\nCurrently, most of the job of a deep learning engineer consists in munging data with Python scripts, then lengthily tuning the architecture \\nand hyperparameters of a deep network to get a working model\\xe2\\x80\\x94or even, to get to a state-of-the-art model, if the engineer is so \\nambitious. Needless to say, that is not an optimal setup. But AI can help there too. Unfortunately, the data munging part is tough to \\nautomate, since it often requires domain knowledge as well as a clear high-level understanding of what the engineer wants to achieve. \\nHyperparameter tuning, however, is a simple search procedure, and we already know what the engineer wants to achieve in this case: it is \\ndefined by the loss function of the network being tuned. It is already common practice to set up basic \"AutoML\" systems that will take care \\nof most of the model knob tuning. I even set up my own years ago to win Kaggle competitions.\\nAt the most basic level, such a system would simply tune the number of layers in a stack, their order, and the number of units or filters \\nin each layer. This is commonly done with libraries such as Hyperopt, which we discussed in Chapter 7 \\n(Note: of Deep Learning with Python). \\nBut we can also be far more \\nambitious, and attempt to learn an appropriate architecture from scratch, with as few constraints as possible. This is possible via \\nreinforcement learning, for instance, or genetic algorithms.\\nAnother important AutoML direction is to learn model architecture jointly with model weights. Because training a new model from scratch \\nevery time we try a slightly different architecture is tremendously inefficient, a truly powerful AutoML system would manage to evolve \\narchitectures at the same time as the features of the model are being tuned via backprop on the training data, thus eliminating all computational redundancy. \\nSuch approaches are already starting to emerge as I am writing these lines.\\nWhen this starts happening, the jobs of machine learning engineers will not disappear\\xe2\\x80\\x94rather, engineers will move higher up the value creation chain. \\nThey will start putting a lot more effort into crafting complex loss functions that truly reflect business goals, \\nand understanding deeply how their models impact the digital ecosystems in which they are deployed \\n(e.g. the users that consume the model\\'s predictions and generate the model\\'s training data)\\n\\xe2\\x80\\x94problems that currently only the largest company can afford to consider.\\nLifelong learning and modular subroutine reuse\\nIf models get more complex and are built on top of richer algorithmic primitives, then this increased complexity will require higher \\nreuse between tasks, rather than training a new model from scratch every time we have a new task or a new dataset. Indeed, a lot datasets \\nwould not contain enough information to develop a new complex model from scratch, and it will become necessary to leverage information \\ncoming from previously encountered datasets. Much like you don\\'t learn English from scratch every time you open a new book\\xe2\\x80\\x94that would be \\nimpossible. Besides, training models from scratch on every new task is very inefficient due to the large overlap between the current tasks and \\npreviously encountered tasks.\\nAdditionally, a remarkable observation that has been made repeatedly in recent years is that training a same model to do several loosely \\nconnected tasks at the same time results in a model that is better at each task. For instance, training a same neural machine translation \\nmodel to cover both English-to-German translation and French-to-Italian translation will result in a model that is better at each language \\npair. Training an image classification model jointly with an image segmentation model, sharing the same convolutional base, results in a \\nmodel that is better at both tasks. And so on. This is fairly intuitive: there is always some information overlap between these seemingly \\ndisconnected tasks, and the joint model has thus access to a greater amount of information about each individual task than a model trained \\non that specific task only.\\nWhat we currently do along the lines of model reuse across tasks is to leverage pre-trained weights for models that perform common \\nfunctions, like visual feature extraction. You saw this in action in Chapter 5. In the future, I would expect a generalized version of this \\nto be commonplace: we would not only leverage previously learned features (submodel weights), but also model architectures and training \\nprocedures. As models become more like programs, we would start reusing program subroutines, like the functions and classes found in \\nhuman programming languages.\\nThink of the process of software development today: once an engineer solves a specific problem (HTTP queries in Python, for instance), they \\nwill package it as an abstract and reusable library. Engineers that face a similar problem in the future can simply search for existing \\nlibraries, download one and use it in their own project. In a similar way, in the future, meta-learning systems will be able to assemble \\nnew programs by sifting through a global library of high-level reusable blocks. When the system would find itself developing similar \\nprogram subroutines for several different tasks, if would come up with an \"abstract\", reusable version of the subroutine and would store it \\nin the global library. Such a process would implement the capability for abstraction, a necessary component for achieving \"extreme \\ngeneralization\": a subroutine that is found to be useful across different tasks and domains can be said to \"abstract\" some aspect of problem-solving. \\nThis definition of \"abstraction\" is similar to the notion of abstraction in software engineering. \\nThese subroutines could be either geometric (deep learning modules with pre-trained representations) \\nor algorithmic (closer to the libraries that contemporary software engineers manipulate).\\n\\nFigure: A meta-learner capable of quickly developing task-specific models using reusable primitives (both algorithmic and geometric), thus achieving \"extreme generalization\".\\nIn summary: the long-term vision\\nIn short, here is my long-term vision for machine learning:\\n\\nModels will be more like programs, and will have capabilities that go far beyond the continuous geometric transformations of the input \\ndata that we currently work with. These programs will arguably be much closer to the abstract mental models that humans maintain about their \\nsurroundings and themselves, and they will be capable of stronger generalization due to their rich algorithmic nature.\\nIn particular, models will blend algorithmic modules providing formal reasoning, search, and abstraction capabilities, with geometric modules \\nproviding informal intuition and pattern recognition capabilities. \\nAlphaGo (a system that required a lot of manual software engineering and human-made design decisions) \\nprovides an early example of what such a blend between symbolic and geometric AI could look like.\\nThey will be grown automatically rather than handcrafted by human engineers, using modular parts stored in a global library of reusable \\nsubroutines\\xe2\\x80\\x94a library evolved by learning high-performing models on thousands of previous tasks and datasets. As common problem-solving \\npatterns are identified by the meta-learning system, they would be turned into a reusable subroutine\\xe2\\x80\\x94much like functions and classes in \\ncontemporary software engineering\\xe2\\x80\\x94and added to the global library. This achieves the capability for abstraction.\\nThis global library and associated model-growing system will be able to achieve some form of human-like \"extreme generalization\": given a \\nnew task, a new situation, the system would be able to assemble a new working model appropriate for the task using very little data, thanks \\nto 1) rich program-like primitives that generalize well and 2) extensive experience with similar tasks. \\nIn the same way that humans can learn to play a complex new video \\ngame using very little play time because they have experience with many previous games, and because the models derived from this previous \\nexperience are abstract and program-like, rather than a basic mapping between stimuli and action.\\nAs such, this perpetually-learning model-growing system could be interpreted as an AGI\\xe2\\x80\\x94an Artificial General Intelligence. But don\\'t \\nexpect any singularitarian robot apocalypse to ensue: that\\'s a pure fantasy, coming from a long series of profound misunderstandings of \\nboth intelligence and technology. This critique, however, does not belong here.\\n\\n@fchollet, May 2017\\n        \\n\\n\\n\\n\\n        \\n                \\n                Powered by pelican, which takes great advantages of python.\\n                \\n        \\n\\n    \\n    var gaJsHost = ((\"https:\" == document.location.protocol) ? \"https://ssl.\" : \"http://www.\");\\n    document.write(unescape(\"%3Cscript src=\\'\" + gaJsHost + \"google-analytics.com/ga.js\\' type=\\'text/javascript\\'%3E%3C/script%3E\"));\\n    \\n    \\n    try {\\n        var pageTracker = _gat._getTracker(\"UA-61785484-1\");\\n    pageTracker._trackPageview();\\n    } catch(err) {}\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "#3. download webpage\n",
    "\n",
    "webpage=str(urllib.request.urlopen('https://blog.keras.io/the-future-of-deep-learning.html').read())\n",
    "soup = bs4.BeautifulSoup(webpage)\n",
    "\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BarContainer object of 26 artists>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#3. frequency of letters\n",
    "\n",
    "LETTERS = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "letterCount = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0, 'F': 0, 'G': 0, 'H': 0, 'I': 0, 'J': 0, 'K': 0, 'L': 0, 'M': 0, 'N': 0, 'O': 0, 'P': 0, 'Q': 0, 'R': 0, 'S': 0, 'T': 0, 'U': 0, 'V': 0, 'W': 0, 'X': 0, 'Y': 0, 'Z': 0}\n",
    "\n",
    "for letter in soup.get_text().upper():\n",
    "    if letter in LETTERS:\n",
    "        letterCount[letter] += 1\n",
    "\n",
    "char_freq= dict(Counter(soup.get_text()))\n",
    "#print(plt.bar(list(char_freq.keys()), char_freq.values()))\n",
    "print(plt.bar(list(letterCount.keys()), letterCount.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n",
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "#4. download sound file\n",
    "\n",
    "r=requests.get('https://d1490khl9dq1ow.cloudfront.net/sfx/mp3preview/seagulls_GJwEHaN_.mp3')\n",
    "for chunk in r.iter_content(chunk_size=255):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
